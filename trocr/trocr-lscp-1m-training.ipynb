{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsTaPrDR7My2"
   },
   "source": [
    "## Prepare data\n",
    "\n",
    "We first download the data. Here, I'm just using the IAM test set, as this was released by the TrOCR authors in the unilm repository. It can be downloaded from [this page](https://github.com/microsoft/unilm/tree/master/trocr). \n",
    "\n",
    "Let's make a [regular PyTorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). We first create a Pandas dataframe with 2 columns. Each row consists of the file name of an image, and the corresponding text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets generate some data. Using the package trdg at this repo [https://github.com/hezarai/trdg-persian](https://github.com/hezarai/trdg-persian) generate a \n",
    "tiny 1 million text images. The preferred command is below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "repo_name=\"trdg-persian\"\n",
    "repo_url=\"https://github.com/hezarai/$repo_name.git\"\n",
    "\n",
    "# Check if the repo exists\n",
    "if [ ! -d \"$repo_name\" ]; then\n",
    "  # Clone the repo if it doesn't exist\n",
    "  git clone \"$repo_url\"\n",
    "else\n",
    "  # Pull the latest changes if the repo exists\n",
    "  cd \"$repo_name\"\n",
    "  git pull\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2722222583.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    meson build\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# install libfreetype6-dev\n",
    "# sudo apt-get -y install libfreetype6-dev\n",
    "# close terminal and open notebook again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: Pillow 8.3.2\n",
      "Uninstalling Pillow-8.3.2:\n",
      "  Successfully uninstalled Pillow-8.3.2\n",
      "Collecting pillow==8.3.2\n",
      "  Downloading Pillow-8.3.2.tar.gz (48.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.8/48.8 MB\u001b[0m \u001b[31m519.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pillow\n",
      "  Building wheel for pillow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pillow: filename=Pillow-8.3.2-cp311-cp311-linux_x86_64.whl size=511712 sha256=a527b6f92e21747586041beeb21c0e3aa311b77a6e3695fe0371afc720ff70f9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tgdmuc3v/wheels/b7/42/63/77ac30adf908e40326191e7134909628bb6e242aac46e749b4\n",
      "Successfully built pillow\n",
      "Installing collected packages: pillow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-image 0.21.0 requires pillow>=9.0.1, but you have pillow 8.3.2 which is incompatible.\n",
      "hezar 0.23.1 requires transformers==4.30.2, but you have transformers 4.32.0.dev0 which is incompatible.\n",
      "pyrlottie 2022.0.2 requires attrs<22,>=21.4.0, but you have attrs 23.1.0 which is incompatible.\n",
      "pyrlottie 2022.0.2 requires Pillow<10,>=9.0.0, but you have pillow 8.3.2 which is incompatible.\n",
      "torchvision 0.15.2 requires pillow!=8.3.*,>=5.3.0, but you have pillow 8.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pillow-8.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets jiwer\n",
    "!pip uninstall -y pillow\n",
    "!pip install --no-cache-dir pillow==8.3.2\n",
    "!pip install -U trdg-persian/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing modules for handwritten text generation.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [18:35<00:00, 896.18it/s]\n"
     ]
    }
   ],
   "source": [
    "!trdg --output_dir ./outputs/bw -l fa -fd trdg-persian/trdg/fonts/fa --dict trdg-persian/trdg/dicts/lscp-raw-1m.txt -t 48 -c 1000000 -w 1 -bl 0 -rbl -m 3,3,3,3 -tc \"#284854,#542828\" -k 5 -rk -na 1 -ws -b 0 -f 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you have to move the outputs folder (`./outputs/bw`) to `data/bw` in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the actual job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"outputs/bw\"  # PROVIDE THE RIGHT PATH TO IMAGE DATA FOLDER\n",
    "encoder_path = \"google/vit-base-patch16-224-in21k\"\n",
    "decoder_path = \"HooshvareLab/roberta-fa-zwnj-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate image/text pairs dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "labels = []\n",
    "for f in os.listdir(data_path):\n",
    "    label = f.split(\"_\")[1].split(\".\")[0]\n",
    "    paths.append(f)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"file_name\": paths, \"text\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66053_ ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá...</td>\n",
       "      <td>ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá ÿ®ŸàÿØ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>860015_ ÿ®ÿßÿ≤€å ÿØÿßÿØŸÜ ŸÖÿ±⁄Ø ÿ¢ÿ≥ŸàŸÜ ŸÖ€åÿ¥ŸáŸàŸÇÿ™€å ÿ®ÿß ŸÑÿ®ÿÆŸÜÿØ ŸÖ...</td>\n",
       "      <td>ÿ®ÿßÿ≤€å ÿØÿßÿØŸÜ ŸÖÿ±⁄Ø ÿ¢ÿ≥ŸàŸÜ ŸÖ€åÿ¥ŸáŸàŸÇÿ™€å ÿ®ÿß ŸÑÿ®ÿÆŸÜÿØ ŸÖ€åÿ¨ŸÜ⁄Ø€åŸÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>861256_ €åÿßÿØ ŸÖÿ±ÿØÿß€å ÿ™Ÿà€å ÿ≤Ÿàÿ±ÿÆŸàŸÜŸá ŸÖ€åÿßŸÅÿ™ŸÖ.jpg</td>\n",
       "      <td>€åÿßÿØ ŸÖÿ±ÿØÿß€å ÿ™Ÿà€å ÿ≤Ÿàÿ±ÿÆŸàŸÜŸá ŸÖ€åÿßŸÅÿ™ŸÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65241_ ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá...</td>\n",
       "      <td>ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá ÿ®ŸàÿØ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>573631_ ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØ...</td>\n",
       "      <td>ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá ÿ®ŸàÿØ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0  66053_ ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá...   \n",
       "1  860015_ ÿ®ÿßÿ≤€å ÿØÿßÿØŸÜ ŸÖÿ±⁄Ø ÿ¢ÿ≥ŸàŸÜ ŸÖ€åÿ¥ŸáŸàŸÇÿ™€å ÿ®ÿß ŸÑÿ®ÿÆŸÜÿØ ŸÖ...   \n",
       "2           861256_ €åÿßÿØ ŸÖÿ±ÿØÿß€å ÿ™Ÿà€å ÿ≤Ÿàÿ±ÿÆŸàŸÜŸá ŸÖ€åÿßŸÅÿ™ŸÖ.jpg   \n",
       "3  65241_ ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá...   \n",
       "4  573631_ ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØ...   \n",
       "\n",
       "                                             text  \n",
       "0     ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá ÿ®ŸàÿØ  \n",
       "1   ÿ®ÿßÿ≤€å ÿØÿßÿØŸÜ ŸÖÿ±⁄Ø ÿ¢ÿ≥ŸàŸÜ ŸÖ€åÿ¥ŸáŸàŸÇÿ™€å ÿ®ÿß ŸÑÿ®ÿÆŸÜÿØ ŸÖ€åÿ¨ŸÜ⁄Ø€åŸÖ   \n",
       "2                    €åÿßÿØ ŸÖÿ±ÿØÿß€å ÿ™Ÿà€å ÿ≤Ÿàÿ±ÿÆŸàŸÜŸá ŸÖ€åÿßŸÅÿ™ŸÖ  \n",
       "3     ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá ÿ®ŸàÿØ  \n",
       "4     ÿÆÿ®ÿ±ŸÜ⁄Øÿßÿ± ÿ™ÿ±⁄©ÿ¥ ÿÆŸÖŸæÿßÿ±Ÿá Ÿæ€åÿ¥ŸàŸÜ€åÿ¥ ÿ±Ÿà ⁄Üÿß⁄© ÿØÿßÿØŸá ÿ®ŸàÿØ  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 998701 entries, 0 to 998700\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   file_name  998701 non-null  object\n",
      " 1   text       998701 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJlVYVal9Ojy"
   },
   "source": [
    "We split up the data into training + testing, using sklearn's `train_test_split` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6qLVT1TPN8Nt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwlEBh6B9RTE"
   },
   "source": [
    "Each element of the dataset should return 2 things:\n",
    "* `pixel_values`, which serve as input to the model.\n",
    "* `labels`, which are the `input_ids` of the corresponding text in the image.\n",
    "\n",
    "We use `TrOCRProcessor` to prepare the data for the model. `TrOCRProcessor` is actually just a wrapper around a `ViTFeatureExtractor` (which can be used to resize + normalize images) and a `RobertaTokenizer` (which can be used to encode and decode text into/from `input_ids`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qO5Q8WYp7DLx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class LSCPSynthDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128, h_flip=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "        self.h_flip = h_flip\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text\n",
    "        file_name = self.df[\"file_name\"][idx]\n",
    "        text = self.df[\"text\"][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image_path = os.path.join(self.root_dir, file_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.h_flip:\n",
    "            image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(\n",
    "            text, padding=\"max_length\", max_length=self.max_target_length, \n",
    "        ).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100\n",
    "            for label in labels]\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        encoding = {\n",
    "            \"pixel_values\": pixel_values.squeeze(),\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzL7C60c-v-B"
   },
   "source": [
    "Let's initialize the training and evaluation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KIa78c2W8uT9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, AutoTokenizer\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-stage1\", size=224)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_path)\n",
    "processor.tokenizer = tokenizer\n",
    "train_dataset = LSCPSynthDataset(root_dir=data_path, df=train_df, processor=processor)\n",
    "eval_dataset = LSCPSynthDataset(root_dir=data_path, df=test_df, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiwZLbMeLCfo",
    "outputId": "61ebe4b6-4bcd-411e-dcf3-ce669bf73246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 798960\n",
      "Number of validation examples: 199741\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p8JfQrx-6EM"
   },
   "source": [
    "Let's verify an example from the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwBNrfD78RA7",
    "outputId": "6dd081e0-d868-460a-9e81-2c48de7a09d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "image, labels = list(train_dataset[0].values())\n",
    "print(image.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN-3pf6T_uRe"
   },
   "source": [
    "We can also check the original image and decode the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "QzgOFgD4_7Kw",
    "outputId": "b7aad103-2dbc-4205-bf16-d62a74023354"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=101x48 at 0x7F1325DAE250>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(os.path.join(train_dataset.root_dir, train_df['file_name'][0])).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMtfkDia-8tQ",
    "outputId": "0bc47221-61d4-4c54-9cdd-e389a16c7445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ÿÆÿß⁄©ÿ≥ÿ™ÿ±€å\n"
     ]
    }
   ],
   "source": [
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    tokenizer.model_input_names = [\"labels\"]\n",
    "    features = tokenizer.pad(\n",
    "            features,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxU7TfoYBvg0"
   },
   "source": [
    "## Train a model\n",
    "\n",
    "Here, we initialize the TrOCR model from its pretrained weights. Note that the weights of the language modeling head are already initialized from pre-training, as the model was already trained to generate text during its pre-training stage. Refer to the paper for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRhvTRrGBIfy",
    "outputId": "de96977c-242a-4d2c-9bdf-7546b3349b74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HooshvareLab/roberta-fa-zwnj-base and are newly initialized: ['roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_path, decoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqNELu3cQix5"
   },
   "source": [
    "Importantly, we need to set a couple of attributes, namely:\n",
    "* the attributes required for creating the `decoder_input_ids` from the `labels` (the model will automatically create the `decoder_input_ids` by shifting the `labels` one position to the right and prepending the `decoder_start_token_id`, as well as replacing ids which are -100 by the pad_token_id)\n",
    "* the vocabulary size of the model (for the language modeling head on top of the decoder)\n",
    "* beam-search related parameters which are used when generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sNNT1XS_CMgl"
   },
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt3G4Ts4-3RL"
   },
   "source": [
    "Next, we can define some training hyperparameters by instantiating the `training_args`. Note that there are many more parameters, all of which can be found in the [documentation](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments). You can for example decide what the batch size is for training/evaluation, whether to use mixed precision training (lower memory), the frequency at which you want to save the model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o1G_rkDBzwid"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=8,\n",
    "    fp16=True,\n",
    "    report_to=None,\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=100,\n",
    "    save_steps=25000,\n",
    "    eval_steps=25000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV6KY53xvOgC"
   },
   "source": [
    "We will evaluate the model on the Character Error Rate (CER), which is available in HuggingFace Datasets (see [here](https://huggingface.co/metrics/cer))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yoXD3_P10DD4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41942/152175726.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8G0R0sPFvfqT"
   },
   "source": [
    "The compute_metrics function takes an `EvalPrediction` (which is a NamedTuple) as input, and should return a dictionary. The model will return an EvalPrediction at evaluation, which consists of 2 things:\n",
    "* predictions: the predictions by the model.\n",
    "* label_ids: the actual ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Y36AcnvP0OZw"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vujH5mZ4-MXS"
   },
   "source": [
    "Let's train! We also provide the `default_data_collator` to the Trainer, which is used to batch together examples.\n",
    "\n",
    "Note that evaluation takes quite a long time, as we're using beam search for decoding, which requires several forward passes for a given example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "mcQMbxi10SDm",
    "outputId": "3750a131-4521-4d24-ccf6-b89622d0757b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/py_3.11/lib/python3.11/site-packages/transformers/models/trocr/processing_trocr.py:135: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='740' max='199744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   740/199744 08:41 < 39:05:22, 1.41 it/s, Epoch 0.03/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMn8k9j37HGBCAplZPQJ1Jp",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1AiB-bjFpcWXp3eRsfXjWFC8-RpvHVQJS",
   "name": "Fine-tune TrOCR on IAM Handwriting Database using Seq2SeqTrainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
